{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc47abc-de54-4393-9d63-869b3d09cd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a81260-4bc0-4a4f-a969-eda3b65bf4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cef494-e9df-492a-81cc-131b294f6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################   DATALOADER    ###########################################\n",
    "class MedicalImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        msk_name = self.mask_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        msk_path = os.path.join(self.mask_dir, msk_name)\n",
    "        img = np.load(img_path)\n",
    "        msk = np.load(msk_path)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        msk = np.expand_dims(msk, axis=0)\n",
    "        subject_id = img_name.split('_')[0]\n",
    "        return {'image': torch.from_numpy(img), 'mask': torch.from_numpy(msk)}\n",
    "\n",
    "test_image_folder = \"/ssd_scratch/ATLAS/Training/test/images\"\n",
    "test_mask_folder = \"/ssd_scratch/ATLAS/Training/test/masks\"\n",
    "test_dataset = MedicalImageSegmentationDataset(test_image_folder, test_mask_folder)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876554d4-1878-4282-939d-ce2405533027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): U_Transformer(\n",
       "    (inc): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (down1): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down2): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (down3): Down(\n",
       "      (maxpool_conv): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): DoubleConv(\n",
       "          (double_conv): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (5): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (MHSA): MultiHeadSelfAttention(\n",
       "      (query): MultiHeadDense()\n",
       "      (key): MultiHeadDense()\n",
       "      (value): MultiHeadDense()\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (pe): PositionalEncodingPermute2D(\n",
       "        (penc): PositionalEncoding2D()\n",
       "      )\n",
       "    )\n",
       "    (up1): TransformerUp(\n",
       "      (MHCA): MultiHeadCrossAttention(\n",
       "        (Sconv): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "        (Yconv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (query): MultiHeadDense()\n",
       "        (key): MultiHeadDense()\n",
       "        (value): MultiHeadDense()\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        )\n",
       "        (Yconv2): Sequential(\n",
       "          (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "          (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (softmax): Softmax(dim=1)\n",
       "        (Spe): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "        (Ype): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (up2): TransformerUp(\n",
       "      (MHCA): MultiHeadCrossAttention(\n",
       "        (Sconv): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "        (Yconv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (query): MultiHeadDense()\n",
       "        (key): MultiHeadDense()\n",
       "        (value): MultiHeadDense()\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        )\n",
       "        (Yconv2): Sequential(\n",
       "          (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (softmax): Softmax(dim=1)\n",
       "        (Spe): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "        (Ype): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (up3): TransformerUp(\n",
       "      (MHCA): MultiHeadCrossAttention(\n",
       "        (Sconv): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "          (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU(inplace=True)\n",
       "        )\n",
       "        (Yconv): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (query): MultiHeadDense()\n",
       "        (key): MultiHeadDense()\n",
       "        (value): MultiHeadDense()\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        )\n",
       "        (Yconv2): Sequential(\n",
       "          (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (4): ReLU(inplace=True)\n",
       "        )\n",
       "        (softmax): Softmax(dim=1)\n",
       "        (Spe): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "        (Ype): PositionalEncodingPermute2D(\n",
       "          (penc): PositionalEncoding2D()\n",
       "        )\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (outc): OutConv(\n",
       "      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2D  U-Net Transformer Architecture\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2,\n",
    "                                  mode='bilinear',\n",
    "                                  align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                in_channels // 2,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(\n",
    "            x1,\n",
    "            [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class MultiHeadDense(nn.Module):\n",
    "    def __init__(self, d, bias=False):\n",
    "        super(MultiHeadDense, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(d, d))\n",
    "        if bias:\n",
    "            raise NotImplementedError()\n",
    "            self.bias = Parameter(torch.Tensor(d, d))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:[b, h*w, d]\n",
    "        b, wh, d = x.size()\n",
    "        x = torch.bmm(x, self.weight.repeat(b, 1, 1))\n",
    "        # x = F.linear(x, self.weight, self.bias)\n",
    "        return x\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "    def positional_encoding_2d(self, d_model, height, width):\n",
    "        \"\"\"\n",
    "        reference: wzlxjtu/PositionalEncoding2D\n",
    "        :param d_model: dimension of the model\n",
    "        :param height: height of the positions\n",
    "        :param width: width of the positions\n",
    "        :return: d_model*height*width position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                             \"odd dimension (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        try:\n",
    "            pe = pe.to(torch.device(\"cuda:0\"))\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(\n",
    "            0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(\n",
    "            0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(\n",
    "            0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(\n",
    "            0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding2D, self).__init__()\n",
    "        channels = int(np.ceil(channels / 2))\n",
    "        self.channels = channels\n",
    "        inv_freq = 1. / (10000\n",
    "                         **(torch.arange(0, channels, 2).float() / channels))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        \"\"\"\n",
    "        :param tensor: A 4d tensor of size (batch_size, x, y, ch)\n",
    "        :return: Positional Encoding Matrix of size (batch_size, x, y, ch)\n",
    "        \"\"\"\n",
    "        if len(tensor.shape) != 4:\n",
    "            raise RuntimeError(\"The input tensor has to be 4d!\")\n",
    "        batch_size, x, y, orig_ch = tensor.shape\n",
    "        pos_x = torch.arange(x,\n",
    "                             device=tensor.device).type(self.inv_freq.type())\n",
    "        pos_y = torch.arange(y,\n",
    "                             device=tensor.device).type(self.inv_freq.type())\n",
    "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
    "        sin_inp_y = torch.einsum(\"i,j->ij\", pos_y, self.inv_freq)\n",
    "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()),\n",
    "                          dim=-1).unsqueeze(1)\n",
    "        emb_y = torch.cat((sin_inp_y.sin(), sin_inp_y.cos()), dim=-1)\n",
    "        emb = torch.zeros((x, y, self.channels * 2),\n",
    "                          device=tensor.device).type(tensor.type())\n",
    "        emb[:, :, :self.channels] = emb_x\n",
    "        emb[:, :, self.channels:2 * self.channels] = emb_y\n",
    "\n",
    "        return emb[None, :, :, :orig_ch].repeat(batch_size, 1, 1, 1)\n",
    "\n",
    "class PositionalEncodingPermute2D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        \"\"\"\n",
    "        Accepts (batchsize, ch, x, y) instead of (batchsize, x, y, ch)\n",
    "        \"\"\"\n",
    "        super(PositionalEncodingPermute2D, self).__init__()\n",
    "        self.penc = PositionalEncoding2D(channels)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        tensor = tensor.permute(0, 2, 3, 1)\n",
    "        enc = self.penc(tensor)\n",
    "        return enc.permute(0, 3, 1, 2)\n",
    "        \n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, channel):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.query = MultiHeadDense(channel, bias=False)\n",
    "        self.key = MultiHeadDense(channel, bias=False)\n",
    "        self.value = MultiHeadDense(channel, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.pe = PositionalEncodingPermute2D(channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        # pe = self.positional_encoding_2d(c, h, w)\n",
    "        pe = self.pe(x)\n",
    "        x = x + pe\n",
    "        x = x.reshape(b, c, h * w).permute(0, 2, 1)  #[b, h*w, d]\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        A = self.softmax(torch.bmm(Q, K.permute(0, 2, 1)) /\n",
    "                         math.sqrt(c))  #[b, h*w, h*w]\n",
    "        V = self.value(x)\n",
    "        x = torch.bmm(A, V).permute(0, 2, 1).reshape(b, c, h, w)\n",
    "        return x\n",
    "        \n",
    "class MultiHeadCrossAttention(MultiHeadAttention):\n",
    "    def __init__(self, channelY, channelS):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.Sconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2), nn.Conv2d(channelS, channelS, kernel_size=1),\n",
    "            nn.BatchNorm2d(channelS), nn.ReLU(inplace=True))\n",
    "        self.Yconv = nn.Sequential(\n",
    "            nn.Conv2d(channelY, channelS, kernel_size=1),\n",
    "            nn.BatchNorm2d(channelS), nn.ReLU(inplace=True))\n",
    "        self.query = MultiHeadDense(channelS, bias=False)\n",
    "        self.key = MultiHeadDense(channelS, bias=False)\n",
    "        self.value = MultiHeadDense(channelS, bias=False)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channelS, channelS, kernel_size=1),\n",
    "            nn.BatchNorm2d(channelS), nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))\n",
    "        self.Yconv2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(channelY, channelY, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(channelY, channelS, kernel_size=1),\n",
    "            nn.BatchNorm2d(channelS), nn.ReLU(inplace=True))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.Spe = PositionalEncodingPermute2D(channelS)\n",
    "        self.Ype = PositionalEncodingPermute2D(channelY)\n",
    "\n",
    "    def forward(self, Y, S):\n",
    "        Sb, Sc, Sh, Sw = S.size()\n",
    "        Yb, Yc, Yh, Yw = Y.size()\n",
    "        # Spe = self.positional_encoding_2d(Sc, Sh, Sw)\n",
    "        Spe = self.Spe(S)\n",
    "        S = S + Spe\n",
    "        S1 = self.Sconv(S).reshape(Yb, Sc, Yh * Yw).permute(0, 2, 1)\n",
    "        V = self.value(S1)\n",
    "        # Ype = self.positional_encoding_2d(Yc, Yh, Yw)\n",
    "        Ype = self.Ype(Y)\n",
    "        Y = Y + Ype\n",
    "        Y1 = self.Yconv(Y).reshape(Yb, Sc, Yh * Yw).permute(0, 2, 1)\n",
    "        Y2 = self.Yconv2(Y)\n",
    "        Q = self.query(Y1)\n",
    "        K = self.key(Y1)\n",
    "        A = self.softmax(torch.bmm(Q, K.permute(0, 2, 1)) / math.sqrt(Sc))\n",
    "        x = torch.bmm(A, V).permute(0, 2, 1).reshape(Yb, Sc, Yh, Yw)\n",
    "        Z = self.conv(x)\n",
    "        Z = Z * S\n",
    "        Z = torch.cat([Z, Y2], dim=1)\n",
    "        return Z\n",
    "        \n",
    "class TransformerUp(nn.Module):\n",
    "    def __init__(self, Ychannels, Schannels):\n",
    "        super(TransformerUp, self).__init__()\n",
    "        self.MHCA = MultiHeadCrossAttention(Ychannels, Schannels)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(Ychannels,\n",
    "                      Schannels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True), nn.BatchNorm2d(Schannels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(Schannels,\n",
    "                      Schannels,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1,\n",
    "                      bias=True), nn.BatchNorm2d(Schannels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, Y, S):\n",
    "        x = self.MHCA(Y, S)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class U_Transformer(nn.Module):\n",
    "    def __init__(self, in_channels=1, classes=1, bilinear=True):\n",
    "        super(U_Transformer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.classes = classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.MHSA = MultiHeadSelfAttention(512)\n",
    "        self.up1 = TransformerUp(512, 256)\n",
    "        self.up2 = TransformerUp(256, 128)\n",
    "        self.up3 = TransformerUp(128, 64)\n",
    "        self.outc = OutConv(64, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x4 = self.MHSA(x4)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "model = U_Transformer(in_channels=1, classes=1)\n",
    "model = DataParallel(model)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('/home/prantik.deb/notebooks/2D_models_demo/best_model_trans_unet.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97617b5a-9180-4f9f-845e-536eed4d99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################         LOSS FUNCTION      ########################################\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, squared_denom=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = sys.float_info.epsilon\n",
    "        self.squared_denom = squared_denom\n",
    "    def forward(self, x, target):\n",
    "        x = x.view(-1)\n",
    "        target = target.view(-1)\n",
    "        intersection = (x * target).sum()\n",
    "        numer = 2. * intersection + self.smooth\n",
    "        factor = 2 if self.squared_denom else 1\n",
    "        denom = x.pow(factor).sum() + target.pow(factor).sum() + self.smooth\n",
    "        dice_index = numer / denom\n",
    "        return 1 - dice_index\n",
    "class BCEWithLogitsAndDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.1, smooth=1.):\n",
    "        super(BCEWithLogitsAndDiceLoss, self).__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.smooth = smooth\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce_loss(inputs, targets)\n",
    "        dice_loss = self.dice_loss(torch.sigmoid(inputs), targets)\n",
    "        loss = self.bce_weight * bce_loss + (1. - self.bce_weight) * dice_loss\n",
    "        return loss.mean()\n",
    "criterion = BCEWithLogitsAndDiceLoss(bce_weight=0.1)\n",
    "\n",
    "def dice_coefficient(inputs, labels, smooth=1):\n",
    "    inputs = inputs.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    intersection = (inputs * labels).sum()\n",
    "    union = inputs.sum() + labels.sum()\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "# IOU\n",
    "def IoU(output, labels):\n",
    "    smooth = 1.\n",
    "    intersection = torch.logical_and(output, labels).sum()\n",
    "    union = torch.logical_or(output, labels).sum()\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6d6f15-7961-4fcf-bc52-1d164c5d78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Dice: 0.5838\n",
      "Average Test IoU: 0.4760\n",
      "Average Test Precision: 0.6425\n",
      "Average Test Recall: 0.5977\n"
     ]
    }
   ],
   "source": [
    "# Saving Results\n",
    "ep3 = []\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_dice = 0.0\n",
    "test_iou = 0.0\n",
    "num_slices = 0\n",
    "test_precision = 0.0\n",
    "test_recall = 0.0\n",
    "if not os.path.exists('/ssd_scratch/ATLAS_2/results_2d/results_trans_unet'):\n",
    "    os.makedirs('/ssd_scratch/ATLAS_2/results_2d/results_trans_unet')\n",
    "with torch.no_grad(): \n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data['image'], data['mask']\n",
    "        inputs = inputs.to('cuda').float()\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()        \n",
    "        batch_dice = []\n",
    "        batch_iou = []\n",
    "        batch_precision = []\n",
    "        batch_recall = []\n",
    "        for j in range(outputs.shape[0]):\n",
    "            dice = dice_coefficient(torch.sigmoid(outputs[j]), labels[j]).item()\n",
    "            iou = IoU(outputs[j] > 0.5, labels[j] > 0.5).item()            \n",
    "            true_positives = torch.sum((outputs[j] > 0.5) & (labels[j] > 0.5)).item()\n",
    "            false_positives = torch.sum((outputs[j] > 0.5) & (labels[j] <= 0.5)).item()\n",
    "            false_negatives = torch.sum((outputs[j] <= 0.5) & (labels[j] > 0.5)).item()            \n",
    "            precision = true_positives / (true_positives + false_positives + 1e-6)\n",
    "            recall = true_positives / (true_positives + false_negatives + 1e-6)            \n",
    "            batch_dice.append(dice)\n",
    "            batch_iou.append(iou)\n",
    "            batch_precision.append(precision)\n",
    "            batch_recall.append(recall)            \n",
    "        test_dice += np.mean(batch_dice)\n",
    "        test_iou += np.mean(batch_iou)\n",
    "        test_precision += np.mean(batch_precision)\n",
    "        test_recall += np.mean(batch_recall)       \n",
    "        # Save the image, ground truth mask, and predicted mask together for comparison\n",
    "        for j in range(len(inputs)):\n",
    "            image = inputs[j].cpu().numpy().transpose((1, 2, 0))\n",
    "            ground_truth_mask = labels[j].cpu().numpy().squeeze()  \n",
    "            predicted_mask = torch.sigmoid(outputs[j]).cpu().numpy() > 0.5\n",
    "            predicted_mask = predicted_mask.squeeze() \n",
    "            plt.figure()\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.title('Image')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(ground_truth_mask, cmap='gray')\n",
    "            plt.title('Ground Truth Mask')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(predicted_mask, cmap='gray')\n",
    "            plt.title('Predicted Mask')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('/ssd_scratch/ATLAS_2/results_2d/results_trans_unet/result_{}_{}.png'.format(i, j), dpi=100)\n",
    "            plt.close()            \n",
    "        #After processing all the batches, the average metrics per slice are computed by dividing the \n",
    "        #accumulated metrics (test_loss, test_dice, test_iou, test_precision, test_recall) by the total \n",
    "        #number of slices (num_slices)\n",
    "    # Calculate average metrics for the test dataset\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    avg_test_dice = test_dice / len(test_dataloader)\n",
    "    avg_test_iou = test_iou / len(test_dataloader)\n",
    "    avg_test_precision = test_precision / len(test_dataloader)\n",
    "    avg_test_recall = test_recall / len(test_dataloader)\n",
    "    # Append epoch metrics to the list\n",
    "    ep3.append([avg_test_loss, avg_test_dice, avg_test_iou, avg_test_precision, avg_test_recall])\n",
    "    # Print the average metrics\n",
    "    print('Average Test Dice: {:.4f}'.format(avg_test_dice))\n",
    "    print('Average Test IoU: {:.4f}'.format(avg_test_iou))\n",
    "    print('Average Test Precision: {:.4f}'.format(avg_test_precision))\n",
    "    print('Average Test Recall: {:.4f}'.format(avg_test_recall))\n",
    "\n",
    "ep_df = pd.DataFrame(np.array(ep3), columns=['Loss', 'Dice', 'IoU', 'Precision', 'Recall'])\n",
    "ep_df.to_csv('metrics_test_trans_unet.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbf6bd-ee2b-416c-85ac-b29a1cc5f989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
